{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. INITIAL SETUP AND LIBRARY IMPORTS\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Install Required Packages ---\n",
    "# obonet is for parsing the Gene Ontology .obo file.\n",
    "# Biopython is for parsing FASTA sequence files.\n",
    "# transformers and sentencepiece are for the protein language model.\n",
    "!pip install -q obonet biopython transformers sentencepiece\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Bioinformatics & Data Handling ---\n",
    "import obonet\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- Machine Learning ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "# --- Hugging Face Transformers for Protein Language Models ---\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set a seed for reproducibility.\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device configuration (CPU is mandated for this notebook).\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Define base path for data.\n",
    "BASE_PATH = \"/kaggle/input/cafa-6-protein-function-prediction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. DATA LOADING AND INITIAL PARSING\n",
    "# ==============================================================================\n",
    "# In this cell, we load all the primary files provided by the competition.\n",
    "\n",
    "print(\"Loading primary data files...\")\n",
    "\n",
    "# --- Load Gene Ontology Graph ---\n",
    "# The obonet library is used to parse the .obo file into a networkx graph\n",
    "go_graph = obonet.read_obo(os.path.join(BASE_PATH, 'Train/go-basic.obo'))\n",
    "print(f\"Gene Ontology graph loaded with {len(go_graph)} nodes and {len(go_graph.edges)} edges.\")\n",
    "\n",
    "# --- Load Training Terms ---\n",
    "train_terms_df = pd.read_csv(os.path.join(BASE_PATH, 'Train/train_terms.tsv'), sep='\\\\t')\n",
    "print(f\"Training terms loaded. Shape: {train_terms_df.shape}\")\n",
    "\n",
    "# --- Load Training Sequences ---\n",
    "# We will parse the FASTA file later when we need the sequences.\n",
    "train_fasta_path = os.path.join(BASE_PATH, 'Train/train_sequences.fasta')\n",
    "print(f\"Training sequences path set: {train_fasta_path}\")\n",
    "\n",
    "# --- Load Test Sequences ---\n",
    "test_fasta_path = os.path.join(BASE_PATH, 'Test/testsuperset.fasta')\n",
    "print(f\"Test sequences path set: {test_fasta_path}\")\n",
    "\n",
    "# --- Load Information Accretion (Weights) ---\n",
    "ia_df = pd.read_csv(os.path.join(BASE_PATH, 'IA.tsv'), sep='\\\\t', header=None, names=['term_id', 'ia_score'])\n",
    "ia_map = dict(zip(ia_df['term_id'], ia_df['ia_score']))\n",
    "print(f\"Information Accretion scores loaded for {len(ia_map)} terms.\")\n",
    "\n",
    "# --- Display a sample of the training terms data ---\n",
    "print(\"\\\\nSample of train_terms.tsv:\")\n",
    "display(train_terms_df.head())\n",
    "\n",
    "# Table 5: Summary of GO Term Distribution in Training Data\n",
    "print(\"\\\\nTable 5: Summary of GO Term Distribution in Training Data\")\n",
    "display(train_terms_df['aspect'].value_counts().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb601ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. DATA PREPROCESSING AND TARGET MATRIX CONSTRUCTION\n",
    "# ==============================================================================\n",
    "# In this cell, we will process the raw training data into a format suitable\n",
    "# for machine learning: a feature matrix (X) and a target matrix (Y).\n",
    "\n",
    "# --- Limit the number of labels for computational efficiency ---\n",
    "# The full GO ontology has ~40,000 terms. Many are extremely rare.\n",
    "# We will focus on the top N most frequent terms to create a manageable problem for a CPU environment.\n",
    "N_LABELS = 40122\n",
    "\n",
    "# --- Identify the top N most frequent GO terms ---\n",
    "top_n_labels = train_terms_df['term'].value_counts().nlargest(N_LABELS).index.tolist()\n",
    "print(f\"Identified the top {N_LABELS} most frequent GO terms.\")\n",
    "\n",
    "# --- Filter the training data to only include these top labels ---\n",
    "train_terms_filtered_df = train_terms_df[train_terms_df['term'].isin(top_n_labels)]\n",
    "print(f\"Filtered training terms. New shape: {train_terms_filtered_df.shape}\")\n",
    "\n",
    "# --- Create a list of unique proteins in our filtered dataset ---\n",
    "unique_proteins = train_terms_df['EntryID'].unique()\n",
    "print(f\"Number of unique proteins with top {N_LABELS} labels: {len(unique_proteins)}\")\n",
    "\n",
    "# --- Create a mapping from protein ID to a list of its GO terms ---\n",
    "# This is a crucial step for creating the multi-label target matrix.\n",
    "protein_to_go_map = train_terms_filtered_df.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "# --- Use MultiLabelBinarizer to create the target matrix Y ---\n",
    "# This converts the list of GO terms for each protein into a binary vector.\n",
    "mlb = MultiLabelBinarizer(classes=top_n_labels)\n",
    "Y = mlb.fit_transform([protein_to_go_map.get(prot, []) for prot in unique_proteins])\n",
    "\n",
    "print(f\"Target matrix Y created with shape: {Y.shape}\")\n",
    "# Y_train.shape will be (number of unique proteins, N_LABELS)\n",
    "\n",
    "# Table 6: Sparsity Analysis of the Target Matrix\n",
    "matrix_density = Y.sum() / (Y.shape[0] * Y.shape[1])\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"Table 6: Sparsity Analysis of the Target Matrix\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of Proteins (Rows): {Y.shape[0]}\")\n",
    "print(f\"Number of GO Terms (Columns): {Y.shape[1]}\")\n",
    "print(f\"Total Annotations: {Y.sum()}\")\n",
    "print(f\"Matrix Density: {matrix_density:.4%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5eab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the pre-embedded protein sequnces \n",
    "X = np.load(\"/kaggle/input/train-set-embedding/train_embeddings.npy\")\n",
    "print(f'X is created with shapes:{X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4652b6",
   "metadata": {},
   "source": [
    "# 5. DEEP LEARNING MODEL: MULTI-LABEL GO TERM PREDICTION\n",
    "\n",
    "We'll implement a PyTorch-based deep neural network for multi-label classification. The architecture includes:\n",
    "\n",
    "- **Input**: Pre-computed protein embeddings (1024-dim from ProtT5)\n",
    "- **Hidden Layers**: Multiple fully-connected layers with dropout for regularization\n",
    "- **Output**: Multi-label predictions for GO terms using sigmoid activation\n",
    "- **Loss**: Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss)\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Batch normalization for training stability\n",
    "- Dropout to prevent overfitting\n",
    "- Separate layers to capture complex patterns in embeddings\n",
    "- Sigmoid activation for independent multi-label prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c096dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL ARCHITECTURE\n",
      "============================================================\n",
      "GOTermPredictor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=512, out_features=26125, bias=True)\n",
      "  )\n",
      ")\n",
      "============================================================\n",
      "Total parameters: 18,131,469\n",
      "Trainable parameters: 18,131,469\n",
      "Input dimension: 1024\n",
      "Output dimension: 26125\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5.1. DEFINE DEEP LEARNING MODEL ARCHITECTURE\n",
    "# ==============================================================================\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GOTermPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Neural Network for Multi-Label GO Term Prediction\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: Protein embeddings (embedding_dim)\n",
    "    - Hidden layers with batch normalization and dropout\n",
    "    - Output: GO term predictions (num_labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, num_labels, hidden_dims=[2048, 1024, 512], dropout=0.3):\n",
    "        super(GOTermPredictor, self).__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        input_dim = embedding_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no activation - we'll use BCEWithLogitsLoss)\n",
    "        layers.append(nn.Linear(input_dim, num_labels))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "embedding_dim = X.shape[1]  # Should be 1024 for ProtT5\n",
    "num_labels = Y.shape[1]      # Number of GO terms\n",
    "\n",
    "model = GOTermPredictor(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_labels=num_labels,\n",
    "    hidden_dims=[2048, 1024, 512],\n",
    "    dropout=0.3\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Input dimension: {embedding_dim}\")\n",
    "print(f\"Output dimension: {num_labels}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68011d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET INFORMATION\n",
      "============================================================\n",
      "Training samples: 70,043\n",
      "Validation samples: 12,361\n",
      "Batch size: 32\n",
      "Training batches: 2189\n",
      "Validation batches: 387\n",
      "\n",
      "Average GO terms per protein:\n",
      "  Training: 6.52\n",
      "  Validation: 6.48\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5.2. CREATE DATASET AND DATALOADERS\n",
    "# ==============================================================================\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for protein embeddings and GO term labels\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = torch.FloatTensor(embeddings)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Split data into train/validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.15, \n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ProteinDataset(X_train, Y_train)\n",
    "val_dataset = ProteinDataset(X_val, Y_val)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=0  # Use 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"\\nAverage GO terms per protein:\")\n",
    "print(f\"  Training: {Y_train.sum(axis=1).mean():.2f}\")\n",
    "print(f\"  Validation: {Y_val.sum(axis=1).mean():.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d9a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Loss function: BCEWithLogitsLoss\n",
      "Optimizer: Adam\n",
      "Initial learning rate: 0.001\n",
      "Weight decay: 1e-5\n",
      "LR scheduler: ReduceLROnPlateau\n",
      "Device: cpu\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5.3. TRAINING SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "# Loss function: Binary Cross-Entropy with Logits\n",
    "# This is ideal for multi-label classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer: Adam with weight decay for regularization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler: Reduce LR when validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss function: BCEWithLogitsLoss\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Initial learning rate: 0.001\")\n",
    "print(f\"Weight decay: 1e-5\")\n",
    "print(f\"LR scheduler: ReduceLROnPlateau\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f59ff298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch 1/20\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca9c002a67049cc82c919bf3479104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_124/1420734376.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_124/1420734376.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_124/1808161691.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5.4. TRAINING LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for embeddings, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 20\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, DEVICE)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"âœ“ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5.5. VISUALIZE TRAINING HISTORY\n",
    "# ==============================================================================\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history['train_loss'], label='Training Loss', marker='o', linewidth=2)\n",
    "plt.plot(history['val_loss'], label='Validation Loss', marker='s', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (BCE)', fontsize=12)\n",
    "plt.title('Training and Validation Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final training loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Improvement: {((history['val_loss'][0] - best_val_loss) / history['val_loss'][0] * 100):.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f472e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5.6. EVALUATION ON VALIDATION SET\n",
    "# ==============================================================================\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Get predictions on validation set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in tqdm(val_loader, desc=\"Generating predictions\"):\n",
    "        embeddings = embeddings.to(DEVICE)\n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        \n",
    "        all_predictions.append(probs.cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "Y_val_pred_proba = np.vstack(all_predictions)\n",
    "Y_val_true = np.vstack(all_labels)\n",
    "\n",
    "print(f\"\\nPrediction shape: {Y_val_pred_proba.shape}\")\n",
    "print(f\"True labels shape: {Y_val_true.shape}\")\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE AT DIFFERENT THRESHOLDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    Y_val_pred = (Y_val_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    micro_f1 = f1_score(Y_val_true, Y_val_pred, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(Y_val_true, Y_val_pred, average='macro', zero_division=0)\n",
    "    samples_f1 = f1_score(Y_val_true, Y_val_pred, average='samples', zero_division=0)\n",
    "    \n",
    "    avg_preds = Y_val_pred.sum(axis=1).mean()\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold:.1f}\")\n",
    "    print(f\"  Micro F1:   {micro_f1:.4f}\")\n",
    "    print(f\"  Macro F1:   {macro_f1:.4f}\")\n",
    "    print(f\"  Samples F1: {samples_f1:.4f}\")\n",
    "    print(f\"  Avg predictions per protein: {avg_preds:.2f}\")\n",
    "    \n",
    "    if micro_f1 > best_f1:\n",
    "        best_f1 = micro_f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"BEST THRESHOLD: {best_threshold} (Micro F1: {best_f1:.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992be3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5.7. DETAILED PERFORMANCE ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Use best threshold for final predictions\n",
    "Y_val_pred_final = (Y_val_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "from sklearn.metrics import precision_score, recall_score, hamming_loss\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL VALIDATION SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall metrics\n",
    "micro_f1 = f1_score(Y_val_true, Y_val_pred_final, average='micro', zero_division=0)\n",
    "macro_f1 = f1_score(Y_val_true, Y_val_pred_final, average='macro', zero_division=0)\n",
    "samples_f1 = f1_score(Y_val_true, Y_val_pred_final, average='samples', zero_division=0)\n",
    "\n",
    "micro_precision = precision_score(Y_val_true, Y_val_pred_final, average='micro', zero_division=0)\n",
    "micro_recall = recall_score(Y_val_true, Y_val_pred_final, average='micro', zero_division=0)\n",
    "\n",
    "print(f\"\\nF1 Scores:\")\n",
    "print(f\"  Micro F1:   {micro_f1:.4f}\")\n",
    "print(f\"  Macro F1:   {macro_f1:.4f}\")\n",
    "print(f\"  Samples F1: {samples_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nPrecision & Recall (Micro):\")\n",
    "print(f\"  Precision: {micro_precision:.4f}\")\n",
    "print(f\"  Recall:    {micro_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Hamming Loss: {hamming_loss(Y_val_true, Y_val_pred_final):.4f}\")\n",
    "print(f\"  Total true labels: {int(Y_val_true.sum())}\")\n",
    "print(f\"  Total predictions: {int(Y_val_pred_final.sum())}\")\n",
    "print(f\"  Correct predictions: {int((Y_val_pred_final * Y_val_true).sum())}\")\n",
    "\n",
    "# Per-protein statistics\n",
    "avg_true = Y_val_true.sum(axis=1).mean()\n",
    "avg_pred = Y_val_pred_final.sum(axis=1).mean()\n",
    "print(f\"\\nAverage GO terms per protein:\")\n",
    "print(f\"  True:      {avg_true:.2f}\")\n",
    "print(f\"  Predicted: {avg_pred:.2f}\")\n",
    "\n",
    "# Distribution of predictions\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "pred_counts = Y_val_pred_final.sum(axis=1)\n",
    "print(f\"  Min predictions: {pred_counts.min()}\")\n",
    "print(f\"  Max predictions: {pred_counts.max()}\")\n",
    "print(f\"  Median predictions: {np.median(pred_counts):.0f}\")\n",
    "print(f\"  Std predictions: {pred_counts.std():.2f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5.8. VISUALIZE PREDICTION QUALITY\n",
    "# ==============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Prediction probability distribution\n",
    "axes[0, 0].hist(Y_val_pred_proba.flatten(), bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Best threshold ({best_threshold})')\n",
    "axes[0, 0].set_xlabel('Prediction Probability', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 0].set_title('Distribution of Prediction Probabilities', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Number of predictions per protein\n",
    "pred_counts = Y_val_pred_final.sum(axis=1)\n",
    "true_counts = Y_val_true.sum(axis=1)\n",
    "\n",
    "axes[0, 1].hist(true_counts, bins=30, alpha=0.5, label='True', color='green', edgecolor='black')\n",
    "axes[0, 1].hist(pred_counts, bins=30, alpha=0.5, label='Predicted', color='orange', edgecolor='black')\n",
    "axes[0, 1].axvline(x=true_counts.mean(), color='green', linestyle='--', linewidth=2)\n",
    "axes[0, 1].axvline(x=pred_counts.mean(), color='orange', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Number of GO Terms per Protein', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 1].set_title('GO Terms per Protein: True vs Predicted', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: True vs Predicted scatter\n",
    "axes[1, 0].scatter(true_counts, pred_counts, alpha=0.4, s=20, color='purple')\n",
    "axes[1, 0].plot([0, true_counts.max()], [0, true_counts.max()], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1, 0].set_xlabel('True Number of GO Terms', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Predicted Number of GO Terms', fontsize=11)\n",
    "axes[1, 0].set_title('True vs Predicted GO Terms per Protein', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Per-label performance (top 20 most frequent labels)\n",
    "label_f1_scores = []\n",
    "for i in range(Y_val_true.shape[1]):\n",
    "    if Y_val_true[:, i].sum() > 0:  # Only calculate for labels that exist\n",
    "        f1 = f1_score(Y_val_true[:, i], Y_val_pred_final[:, i], zero_division=0)\n",
    "        label_f1_scores.append((i, f1, Y_val_true[:, i].sum()))\n",
    "\n",
    "# Sort by frequency and take top 20\n",
    "label_f1_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "top_20_labels = label_f1_scores[:20]\n",
    "\n",
    "indices = [x[0] for x in top_20_labels]\n",
    "f1_scores = [x[1] for x in top_20_labels]\n",
    "frequencies = [x[2] for x in top_20_labels]\n",
    "\n",
    "x_pos = np.arange(len(indices))\n",
    "axes[1, 1].bar(x_pos, f1_scores, color='teal', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Top 20 Most Frequent GO Terms (by index)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('F1 Score', fontsize=11)\n",
    "axes[1, 1].set_title('Per-Label F1 Score (Top 20 Labels)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 1].set_xticks(x_pos[::2])  # Show every other label\n",
    "axes[1, 1].set_xticklabels(x_pos[::2], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage F1 score across top 20 labels: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bbeb1",
   "metadata": {},
   "source": [
    "# 6. MODEL COMPARISON & SUMMARY\n",
    "\n",
    "Let's compare the deep learning model with other approaches and provide recommendations for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f23667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6.1. MODEL ARCHITECTURE SUMMARY & RECOMMENDATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEEP LEARNING MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“Š MODEL ARCHITECTURE:\")\n",
    "print(f\"  â€¢ Input dimension: {embedding_dim} (ProtT5 embeddings)\")\n",
    "print(f\"  â€¢ Hidden layers: [2048 â†’ 1024 â†’ 512]\")\n",
    "print(f\"  â€¢ Output dimension: {num_labels} GO terms\")\n",
    "print(f\"  â€¢ Total parameters: {total_params:,}\")\n",
    "print(f\"  â€¢ Regularization: Batch Normalization + Dropout (0.3)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ TRAINING CONFIGURATION:\")\n",
    "print(f\"  â€¢ Loss function: BCEWithLogitsLoss\")\n",
    "print(f\"  â€¢ Optimizer: Adam (lr=0.001, weight_decay=1e-5)\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  â€¢ Epochs trained: {NUM_EPOCHS}\")\n",
    "print(f\"  â€¢ Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ PERFORMANCE METRICS:\")\n",
    "print(f\"  â€¢ Micro F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"  â€¢ Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"  â€¢ Precision (micro): {micro_precision:.4f}\")\n",
    "print(f\"  â€¢ Recall (micro): {micro_recall:.4f}\")\n",
    "print(f\"  â€¢ Optimal threshold: {best_threshold}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
    "print(\"  1. Fine-tune ProtT5 embeddings (currently using pre-computed)\")\n",
    "print(\"  2. Implement hierarchical loss considering GO DAG structure\")\n",
    "print(\"  3. Use focal loss to handle label imbalance\")\n",
    "print(\"  4. Try attention mechanisms to focus on important embedding regions\")\n",
    "print(\"  5. Ensemble multiple models trained with different random seeds\")\n",
    "print(\"  6. Implement per-ontology models (BPO, MFO, CCO separately)\")\n",
    "print(\"  7. Add graph neural network layers to leverage GO hierarchy\")\n",
    "print(\"  8. Use information accretion (IA) scores as sample weights\")\n",
    "\n",
    "print(\"\\nðŸ”§ HYPERPARAMETER TUNING SUGGESTIONS:\")\n",
    "print(\"  â€¢ Hidden layer dimensions: Try [1024, 512, 256] or [3072, 2048, 1024]\")\n",
    "print(\"  â€¢ Dropout rate: Experiment with 0.2-0.5\")\n",
    "print(\"  â€¢ Learning rate: Try 5e-4, 1e-3, 2e-3\")\n",
    "print(\"  â€¢ Batch size: Test 16, 64, 128\")\n",
    "print(\"  â€¢ Add residual connections for deeper networks\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
